\section{Periodic Orbits as Solutions of System of Equations, the Galerkin Operator}

%stationary? ordinary?
Given a system of $n \in \N$ possibly non-linear, autonomous, ordinary, first-order differential equations $\mathbf{x}$
	\begin{equation} \label{eq:SDE}
		\frac{d\mathbf{x}}{dt} = \mathbf{f}(t, \mathbf{x}) \text,
	\end{equation}
we are interested in numerically computed, periodic solutions.
That is, solutions $\mathbf{y}: \R \to \C^n$ which obey $\mathbf{y}(t) = \mathbf{y}(t+T)$ for all $t \in \R$ and some period $T \in \R$.
This is the general case, as differential equations of any degree can be converted to a system of first-order differential equations.

\paragraph{Model} Solution candidates need to be modeled in a certain way.
The periodicity constraint suggests using a multidimensional trigonometric polynomial of degree $m \in \N$
	\[
		\mathbf{y} \coloneqq \sum_{k = -m}^m \mathbf{y}_k \exp\left(i \omega k t\right) \text{,}
	\]
where $\mathbf{y}_k \in \C^n$, $\omega = \frac{2\pi}T$. %$\mathbf{y}_{-k} = \Re(\mathbf{y}_k) - i \Im(\mathbf{y}_k)$ for $k \in \N$, $-m \le k \le m$,
Solution candidates of this form satisfy $\mathbf{y}(t) = \mathbf{y}(t+T)$ by definition.
%A function of this form is defined solely by its $m+1$ unique coefficients $\mathbf{y}_k$ for $0 \le k \le m$.

\paragraph{Optimality Criterion} Finding good solutions, that is, functions $\mathbf{y}$ which at least approximate $\mathbf{y}^\prime = \mathbf{f}(t, \mathbf{y})$, requires a way of judging whether a solution candidate is indeed a correct solution.
In this case, Galerkin's method takes this role.
A useful property of the trigonometric polynomial is, that it can be trivially differentiated
	\begin{equation}
			\frac{d\mathbf{y}}{dt} = \sum_{k = -m}^m i \omega k \mathbf{y}_k \exp\left(i \omega k t\right) \text.
	\label{eq:trigondiff}
	\end{equation}
Employing this property in the definition of the differential equation system yields
	\begin{align*}
			& \frac{d\mathbf{y}}{dt} = \mathbf{f}(t,\mathbf{y})\\
		\Leftrightarrow\ & \mathbf{f}(t,\mathbf{y}) - \frac{d\mathbf{y}}{dt} = 0\\
		\Leftrightarrow\ & \mathbf{f}(t,\mathbf{y}) - \sum_{k = -m}^m i \omega k \mathbf{y}_k \exp\left(i \omega k t\right) = 0 \text.
	\end{align*}
The difference between these two functions is called the \emph{residual} $\mathbf{r}(t) \coloneqq \mathbf{f}(t,\mathbf{y}) - \frac{d\mathbf{y}}{dt}$.
A candidate $\mathbf y$ is a solution if and only if $\mathbf r = 0$.
%Checking for $\mathbf{r}(t) = 0$ would require comparing the two functions at infinitely many points. %TODO fix... not changed by galerkin
The solution of the system of differential equations will generally not be representable by a trigonometric polynomial.
The residual can thus not genuinely equal zero.

\paragraph{Galerkin's Method} Galerkin's method relaxes the equality requirement such that only projections onto a set of so called \emph{trial vectors}, need to vanish.
This is equivalent to requiring a projection of $\mathbf{r}$ onto the subspace spanned by the trial vectors to be zero.
Choosing the complex oscillations as a basis for this subspace as well is a solid choice.
Basically the residual is approximated by a trigonometric polynomial and a solution is required to only minimize this representation.
There are other factors supporting this choice: The residual is periodic as well, because of orthogonality many terms can cancel each other out, it allows us to employ the FFT for many operations, and the resulting system of equations is almost balanced.

\paragraph{System of Equations} This yields $N = 2m+1$ equations, one for each trial vector $\mathbf{v}_k = \exp\left( i \omega k t \right)$ for $-m \le k \le m$
	\begin{align}
		& \langle \mathbf{r} , \mathbf{v}_k \rangle \nonumber \\
		=\ & \langle \mathbf{f}(t,\mathbf{y}), \mathbf{v}_k \rangle - \left\langle \frac{d\mathbf{y}}{dt}, \mathbf{v}_k \right\rangle \nonumber \\
		=\ & \langle \mathbf{f}(t,\mathbf{y}), \mathbf{v}_k \rangle - i \omega k \mathbf{y}_k \label{eq:innerprod} \text.
	\end{align}
The last step exploits that the complex exponential functions are orthonormal.
%The last step exploits that there is always exactly one component in the candidate function' derivative which is not orthogonal to the trial vectors.
For these $N$ equations, there are $N+1$ variables: $N$ unique coefficients and $\omega$.
This represents the situation, that at this point there is still one degree of freedom: Each phase shifted version of a solution is still a solution.
We thus introduce another generic equation called the \emph{anchor} equation, which basically chooses one of these solutions.
In this case $\langle \mathbf{y}(0), (\delta_{1i})_{i \in \N_n} \rangle = 0$ is used:
For $t = 0$, the solution needs to intersect the hyperplane defined by being zero in the first component.
This can be formulated by requiring the corresponding coefficients to sum up to zero.
The anchor equation needs to be adapted to the system considered: If there are no intersections with this plane, another equation needs to be chosen.

%motivation tedious, error prone
\paragraph{Discretization} When transferring these theoretical constructs to a practical setting, the main change is that solutions $\mathbf{y}$ are not considered to be functions of continuous time, but vectors representing discrete time.
Consequently, solution candidates are represented by linear combinations of discrete vectors as well
	\[
		\mathbf{\mathbf{y}} = \left( \sum_{k=0}^{N-1} \mathbf{y}_k \exp\left(i \frac{2\pi}{N} j k\right) \right)_{0 \le j \le N-1} \text.
	\]
This is no real limitation at that point, as solution candidates are band limited by construction.
Define the \emph{DFT matrix} $\mathbf F \in \R^{N \times N}$, the coefficient vector $\mathbf Y \in \left(\C^n\right)^N$ and a diagonal matrix $\mathbf K \in \R^{N\times N}$
	\begin{align*}
		\mathbf{F} &\coloneqq N^{-1} \left( \exp\left(-i \frac{2\pi}{N} j k \right) \right)_{0 \le j,k \le N-1} \\
		\mathbf{Y} &\coloneqq \left(\mathbf{y}_k\right)_{0 \le k \le N-1} \\
		\mathbf K &\coloneqq \diag\left( [0:m]\, ||\, [-m:-1] \right) \text.
	\end{align*}
$\mathbf Y$ is a vector of vectors, while somewhat unusual, this allows for more concise notation later on, than notation as a matrix.
Using these constructs allows us to reformulate
	\[
			\mathbf{y} = \mathbf{F^{-1}} \mathbf{Y} \text.
	\]
Considering only the equations from projections onto trial vectors, and considering $\omega$ to be known the system of equations becomes %TODO define
	\[
		\mathbf{F} \mathbf{f}\{\mathbf F^{-1} \mathbf{Y}\} - i \omega \mathbf K \mathbf Y = 0\text,
	\]
%where $\mathbf{\tilde f}(\mathbf Y) = \mathbf f\{\mathbf Y\}$ simply row wise applies $\mathbf{f}$, the function defining the system of differential equations (see \autoref{eq:SDE}).
where $\mathbf{f}$ defines the system of differential equations (see \autoref{eq:SDE}).

Compare this to \autoref{eq:innerprod}.
Essentially, $\mathbf{f}\{\mathbf F^{-1} \mathbf{Y}\}$ is a sampled version of $\mathbf f(t,\mathbf y)$.
Multiplication by $\mathbf F$ corresponds to calculating the inner products with the trial vectors, which are the rows of $\mathbf F$.
The term $i \omega \mathbf K \mathbf Y$ corresponds to \autoref{eq:trigondiff}, focusing on the coefficients only.

Introducing these new constructs might at first glance appear unnecessarily complicated, because the system of equations was already known, and could be used in the state it was in.
However, an implementation in the basic form would have required different treatment for each dynamic system, while in this form, it is only trivially dependent of $\mathbf f$ and the degree of the trigonometric polynomial.
%That is, the auxiliary vectors and matrices are trivially constructed, as is $\mathbf{\tilde f}$ from $\mathbf f$.
While knowing the system of equations is important, the core task of finding a good solution requires the Jacobian of the system.
The large advantage is, that from this form the system can be derived in a general way.

\paragraph{Deriving the System} %Let $D$ denote a differential operator.
The only variable in the system is $\mathbf Y$, thus
	\begin{align}
		& \frac{d}{d\mathbf Y} \left( \mathbf{F} \mathbf{f}\{\mathbf{F}^{-1} \mathbf{Y}\} - i \omega \mathbf K \mathbf Y \right) \nonumber \\
		=\ & \mathbf{F} \frac{d \mathbf{f}\{\mathbf Y\}}{d\mathbf Y} \left( \mathbf{F}^{-1} \mathbf{Y} \right) \mathbf{F}^{-1} - i \omega \mathbf K (\delta_{ij} \mathbf I_n)_{i,j \in \N_N} \label{eq:vecsys} \text,
	\end{align}
%TODO careful: dY/dY != I
% where $\mathbf J_{\mathbf{\tilde f}} \coloneqq \frac{d \mathbf{f}\{\mathbf Y\}}{d\mathbf Y}$ is the Jacobian of $\mathbf{\tilde f}$, a vector-by-vector derivative, which yields a matrix.
where $\frac{d \mathbf{f} \{ \mathbf Y \}}{d \mathbf Y}$ is a vector-by-vector derivative, which yields a matrix.
However, because both vector's elements are vectors themselves, the entries of the matrix are again vector-by-vector derivatives.
This results in an $N \times N$ matrix of matrices with
	\[
			\left( \frac{d \mathbf{f}\{\mathbf Y\}}{d\mathbf Y} \right)_i^j = \frac{d}{d\mathbf Y_j} \mathbf f (\mathbf Y_i) = \delta_{ij} \frac{d \mathbf f(\mathbf x)}{d \mathbf x}(\mathbf Y_i) \text, %TODO fix x
	\]
where $\frac{d \mathbf f(\mathbf x)}{d\mathbf x}$ is the Jacobian of $\mathbf f$.
% The matrix $\mathbf J_{\mathbf{\tilde f}}$ is diagonal, thus
% 	\[
% 		\mathbf J _ \mathbf{\tilde f} (\mathbf Y) = \diag\left( \left(\mathbf J _ \mathbf f \left(\mathbf Y_i\right) \right)_{i \in \N_N} \right)
% 	\]
At no point different cells of $\frac{d \mathbf f(\mathbf x)}{d\mathbf x}$ interact.
Hence, the expression for the Jacobian of the whole system can be considered separately for each partial derivative in the Jacobian of $\mathbf f$.
For $k,l \in \N_n$ define the $N \times N$ diagonal matrix which extracts a single partial derivative
	\[
		\mathbf A_{kl}(\mathbf Y) = \diag\left( \left(\frac{\partial f_k}{\partial x_l} \left(\mathbf Y_i\right)\right)_{i \in \N_N} \right) \text.
	\]
For each $k$ and $l$ \autoref{eq:vecsys} then becomes
	\begin{equation}
		\mathbf{F} \mathbf A_{kl}(\mathbf{F}^{-1} \mathbf{Y}) \mathbf{F}^{-1} - i \delta_{kl} \omega \mathbf K
	\label{eq:syssingle}
	\end{equation}
This expression is again free of nested vectors.

%Because of the diagonal structure of the matrix, the term $\frac{d\mathbf{\tilde f}}{d\mathbf x}(\mathbf{F}^{-1} \mathbf{Y}) \mathbf{F}^{-1}$ is essentially a multiplication of each row of $\mathbf F^{-1}$ with a constant.
Because $\mathbf A_{kl}(\mathbf{F}^{-1} \mathbf{Y})$ is diagonal, $\mathbf A_{kl}(\mathbf{F}^{-1} \mathbf{Y}) \mathbf{F}^{-1}$ is the rows of $\mathbf F^{-1}$ each multiplied by a constant.
This can also be seen as an element wise multiplication of the columns $\mathbf c_k$ of $\mathbf F^{-1}$ by a column vector $\mathbf d$ defined by the diagonal of $\mathbf A_{kl}(\mathbf{F}^{-1} \mathbf{Y})$
	\begin{align*}
		&\mathbf c_k \coloneqq \left(\exp\left(i\frac{2\pi}{N} j k\right)\right)_{0 \le j \le N-1}\\
		&\mathbf d \coloneqq \frac{\partial f_k}{\partial x_l} \{\mathbf Y\} \text.
	\end{align*}
A column of $\mathbf{F} \mathbf A_{kl}(\mathbf{F}^{-1} \mathbf{Y}) \mathbf{F}^{-1}$ is then the DFT of the product of two signals $\mathcal F(\mathbf d \cdot \mathbf c_k)$.
Because of the structure of $\mathbf c_k$, this corresponds to a simple periodic shift of the Fourier coefficients of the discrete signal $\mathbf d$ by $k$ steps.
Using this for every column of the complete term yields
	\[
			\mathbf{F} \mathbf A_{kl}(\mathbf{F}^{-1} \mathbf{Y}) \mathbf{F}^{-1} = \left( \mathbf \F(\mathbf d)_{((i-j))_N} \right)_{0 \le i,j \le N} \text,
	\]
that is, a simple circulant matrix.

Rebuilding the complete derivative from these matrices is a matter of subtracting the remaining diagonal term and merging the results.
% The general form of \autoref{eq:syssingle} is $\mathbf G = \mathbf M \cdot D \mathbf Y$.
%The differential operator $D$ has been left undefined and \autoref{eq:syssingle} most of the term is independent of the choice of $D$.
%This is important, because in an optimization setting real and imaginary parts of the Fourier coefficients need to be treated separately.
% That is, derivatives of $\frac{d \Re \mathbf G}{d\Re \mathbf Y}$, $\frac{d \Im \mathbf G}{d\Re \mathbf Y}$, $\frac{d \Re \mathbf G}{d\Im \mathbf Y}$, $\frac{d \Im \mathbf G}{d\Im \mathbf Y}$ will be required.
Since a central part of this project requires numerical optimization of the coefficients, their real and imaginary parts need to be treated separately.
This means especially separately deriving the real and imaginary parts of system of equations by their real and imaginary parts.
However, all these cases can be reduced to $\frac{d}{d \mathbf Y}$
\begin{align*}
	\frac{d \Re}{d\Re \mathbf Y} = \Re \frac{d}{d \mathbf Y} &&& \frac{d \Re}{d\Im \mathbf Y} = -\Im \frac{d}{d \mathbf Y}\\
	\frac{d \Im}{d\Re \mathbf Y} = \Im \frac{d}{d \mathbf Y} &&& \frac{d \Im}{d\Im \mathbf Y} = \Re \frac{d}{d \mathbf Y} \text.
\end{align*}
